\section{Conventional Methods in Action Recognition}

This section provides a description of conventional methods in action recognition, which do not leverage deep learning techniques and mostly rely on the extraction of hand-crafted features from input-videos.
Due to the availability of several high-quality survey publications in this area, as described in the related work section \ref{sec:relatedwork}, we provide a condensed overview of these methods by describing the main research directions using the taxonomy of \textcite{aggarwal_human_2011} released in 2011.
For a more detailed description of specific approaches in this area we also refer to \cite{aggarwal_human_2011}.
Additionally we focus on the Bag of Visual Words (BoVW) paradigm, which has become the standard approach in action recognition using hand-crafted features since.
We furthermore identify state-of-the art approaches, that employ this standard approach and describe them in more detail.

%---------------------------------------

3 Main components in action recognition using local features: Feature Extraction, Representation Building, Classification.

Methods for feature extraction: Interest point detectors or dense sampling.

Space-time interest point detectors: Harris3D\cite{laptev_space-time_2005}, Cuboids\cite{dollar_behavior_2005}, Hessian Detector\cite{willems_efficient_2008}

Descriptors for 3D volumes around previously detected space-time interest points: Histogram of Gradient HOG\cite{dalal_histograms_2005-1}, Histogram of Optical Flow (HOF)\cite{laptev_learning_2008}, 3D Histogram of Gradient (HOG3D)\cite{klaser_spatio-temporal_2008}, Extended SURF (ESURF)\cite{willems_efficient_2008}

``standard approach to video classification'' described in \cite{karpathy_large-scale_2014}

Laptev and Lindeberg [26] proposed spatio-temporal interest points (STIPs)
by extending Harris corner detectors to 3D. SIFT and HOG
are also extended into SIFT-3D [34] and HOG3D [19] for
action recognition. Dollar et al. proposed Cuboids features
for behavior recognition [5]. Sadanand and Corso built Ac-
tionBank for action recognition [33]. Recently, Wang et al.
proposed improved Dense Trajectories (iDT) [44] which is
currently the state-of-the-art hand-crafted feature. Tran 2015

Recently, interest point detectors and local descriptors have
been extended from images to videos. Laptev and Linde-
berg [13] introduced space-time interest points by extend-
ing the Harris detector. Other interest point detectors in-
clude detectors based on Gabor filters [1, 5] or on the de-
terminant of the spatio-temporal Hessian matrix [33]. Fea-
ture descriptors range from higher order derivatives (local
jets), gradient information, optical flow, and brightness in-
formation [5, 14, 24] to spatio-temporal extensions of image descriptors, such as 3D-SIFT [25], HOG3D [11], extended
SURF [33], or Local Trinary Patterns [34]. Action Recognition by dense trajectories -- Wang 2011


\subsection{Taxonomy-based Overview}

Approach-based taxonomy

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img_conventional/taxonomy_conventional_methods.png}
    \caption{Approach-based taxonomy for conventional methods in human action recognition as given by Aggarwal and Ryoo \cite{aggarwal_human_2011}}
    \label{fig:conventional_taxonomy}
\end{figure}

Divide the field of activity/action recognition into:
\begin{description}
    \item[Single-layered Approaches] recognize an action directly from raw video-data, i.e.\ based on sequences of video frames.
    \item[Hierarchical Approaches] model an action as a sequence of explicitly defined and individually recognized atomic sub-actions.
\end{description}

2nd level of distinction (single layer):
\begin{description}
    \item[Space-time approaches] interpret a video as a 3D space-time volume, that results from stacking the individual video-frames along the temporal dimension..
    \item[Sequential approaches] interpret a video as a sequence of observations, i.e.\ feature vectors extracted from individual frames.
\end{description}

2nd Level distinction hierarchical approaches:
\begin{description}
    \item[Statistical approaches] construct hierarchically stacked statistical state-based models, such as layerd Hidden Markov Models.
    \item[Syntactic approaches] use a grammar syntax such as stochastic context-free grammar.
    \item[Description-based approaches] describe atomic sub-actions and their temporal, spatial and logical structure.
\end{description}


\subsubsection{Space-time Approaches}
Space-time approaches can be further distinguished by what kind of features from the video volume they use.
Space-time volume approaches employ the video volume or parts of it directly for creating a representation of the video, which is then compared with other video volume representations.
Trajectory approaches use motion trajectories of tracked points inside the volume for the recognition of ongoing actions.
Space-time feature approaches extract features around interest-points locally and aggregate them into a representation of the video volume.


\textbf{Action recognition using space-time volumes} \\
A typical approach for action recognition with space-time volumes uses template matching:
Given a similarity measure for video volumes, the algorithm constructs or selects template video volumes from the training dataset for each action class that has to be recognized.
The template video volumes then act as representations for the action classes.
When presented with a test-video, the algorithm constructs the representation for the new video and compares it to the training templates by using the similarity measure.
The action class, that corresponds to the most similiar training template is selected as output class.

Approaches in this category mainly differ by how a representation is built and how they are compared.
\textcite{bobick_recognition_2001} create templates from the raw video volumes by stacking the silhouettes of persons that perform an action into 2D images.
The resulting binary \textit{motion-energy image} and skalar-valued \textit{motion-history image}, as displayed in figure \ref{fig:spacetimevolumes_meimhi}, are then classified by template matching as described above.

\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{img_conventional/spacetimevolumes_meimhi}
    \caption{Motion-energy image (MEI) and motion-history image (MHI) and example frame of three ballet actions \cite{bobick_recognition_2001}}
    \label{fig:spacetimevolumes_meimhi}
\end{figure}

\textbf{Action recognition using trajectories} \\
The underlying idea of trajectory-based approaches is, that the motion of a person's joint positions are sufficient for recognizing the performed action \cite{johansson_visual_1975}.
Algorithms that follow this approach use space-time trajectories to represent an action.
More specifically the joint positions of a person are tracked in the video volume while performing an action.
The resulting trajectories then represent the performed action and can be used for classification, by either comparing the trajectories directly or by extracting features along the trajectories.

\textcite{sheikh_exploring_2005} classified actions by using the trajectories of $13$ tracked points directly as displayed in figure \ref{fig:trajectories_sheikh}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img_conventional/trajectories_sheikh}
    \caption{Trajectories of a person's tracked joint positions while performing an action. Trajectories shown in XYZ space (a) and XYT space (b) \cite{sheikh_exploring_2005}}
    \label{fig:trajectories_sheikh}
\end{figure}


\textbf{Action recognition using space-time features}
Ohhhh yeeeeeeeeeeeahhhH!!!!

\subsubsection{Sequential Approaches}
In single-layered sequential approaches an action is processed as a sequence of observations, specifically as a sequence of feature vectors.
Therefore, as a first step in sequential approaches, feature vectors need to be extracted from each frame in the video that contains the action.
\textcite{aggarwal_human_2011} describe using the degrees of joint-angles as suitable feature vectors to describe the status of a person while performing an action.
Given the sequence of feature vectors, sequential approaches usually calculate the likelihood of action classes producing the observed sequence of feature vectors.
The action class with the highest likelihood is assigned to input video, i.e.\ to the sequence of feature vector observations.

\textcite{aggarwal_human_2011} further differentiate sequential approaches into exemplar-based and state model-based approaches.

\textbf{Exemplar-based approaches}
Exemplar-based approaches store template sequences of feature vectors for each action class (exemplars).
A presented unkown action in an input video is then recognized, by comparing its sequence of feature vectors to the stored templates.
The action class, whose template is most similar to the feature sequence of the input video is assigned to the input.
The approach has to take into account, that the feature sequences may vary because of different execution styles of action among different persons.
The Dynamic Time Warping algorithm (DTW) has been used widely for matching varying sequences in sequential exemplar-based approaches\cite{darrell_space-time_1993}\cite{gavrila_towards_1995}\cite{veeraraghavan_function_2006}.

\textbf{State model-based approaches}
Sequential state model-based approaches construct models, which are trained to generate sequences of feature vectors.



\subsubsection{Hierarchical Approaches}
The main idea of hierarchical approaches is to model complex actions as a hierarchy of simpler sub-actions \cite{aggarwal_human_2011}.
Sub-actions themselves can be further decomposed, until the initial complex action is represented as a sequence of non-decomposable atomic sub-actions.
A complex action is interpreted as a process that generates sub-actions which can be observed and classified individually.
Most hierarchical approaches thereby employ non-hierarchical single-layered action recognition approaches to recognize the observable low-level sub-actions.

\textcite{aggarwal_human_2011} further differentiate hierarchical approaches into statistical approaches, syntactic approaches and desciption-based approaches.

\textbf{Statistical approaches} \\
Hierarchical statistical approaches use stacked state-based models such as Hidden Markov Models (HMMs) \cite{oliver_layered_2002}\cite{zhang_modeling_2004} or Dynamic Bayesian Networks (DBNs) \cite{dai_group_2008}\cite{gong_recognition_2003} for action recognition.
Typically two layers of such models are used, where the bottom layer recognizes simple actions from sequences of feature vectors and the top layer recognizes high-level actions from the resulting sequence of simple actions.
The layered hidden markov model approach of \textcite{oliver_layered_2002} is said to be one of the most fundamental forms of hierarchical statistical approaches \cite{aggarwal_human_2011}.

\textbf{Syntactic approaches} \\
In hierarchical syntactic approaches, a high-level action is represented as a string of symbols.
Each symbol therein corresponds to a simpler, possibly atomic, sub-action as described previously.
Equivalently to hierarchical statistical approaches, syntactic approaches require the recongition of sub-actions by using any of the previously described methods in order to obtain the string of symbols.
An action class is represented as a set of production rules from context-free grammars or stochastic context-free grammars \cite{ivanov_recognition_2000}\cite{moore_recognizing_2002}\cite{minnen_expectation_2003}\cite{joo_attribute_2006}, that generate sequences of symbols corresponding to the action class.
Syntactic approaches then use parsing techniques from the field of programming languages \cite{hopcroft_introduction_1979} to recognize high-level actions.

\textbf{Description-based approaches} \\
The definition of description-based approaches by \textcite{aggarwal_human_2011} states: ``A description-based approach is a recognition approach, that explicitly maintains human activities' spatio-temporal structure.''
High-level actions are represented by occurences of their underlying sub-actions, while temporal, spatial and logical relationships between the sub-actions are explicitly specified.
The temporal relations between sub-actions are usually specified by associating a time interval with an occuring action.
\textcite{allen_maintaining_1983}\cite{allen_actions_1994} introduced seven predicates to describe temporal relations between time intervals: \textit{before, meets, overlaps, during, starts, finishes} and \textit{equals}, which have been widely used for hierarchical description-based approaches \cite{pinhanez_human_1998}\cite{siskind_grounding_2001}\cite{nevatia_hierarchical_2003}\cite{ryoo_recognition_2006}.


\subsection{Bag of Visual Words}

Defintion of local features: ``primitive events corresponding to moving two-dimensional image structures at moments of non-constant motion'' Schuldt 2004 KTH paper

Interest point detection: Space-time interest points ``I. Laptev. On space-time interest points. IJCV, 64(2-3), 2005.''

Local feature approaches extract features, i.e\ different characteristics of pixel values of a video, in locally limited neigbourhoods.
The algorithm to extract these features is called a feature extractor.

Three main questions:
\begin{enumerate}
    \item Where to the features (around what points?)
    \item What features to extract?
    \item How to aggregate the extracted feature (vectors) into a global fixed-size representation of the video.
\end{enumerate}

``standard approach'' karpathy large-scale

\subsubsection{Feature Extraction}

Cuboids ``P. Dollár, V. Rabaud, G. Cottrell, and S. Belongie. Behavior recognition via sparse spatio-temporal features. In VS-PETS, 2005.''

\subsubsection{Feature Encoding}

\textbf{Bag of visual Words paradigm}

Can be improved with a multi-channel approach as in ``M. M. Ullah, S. N. Parizi, and I. Laptev. Improving bag-of-features action recognition with non-local cues. In BMVC,2010.''

\textbf{Fisher Vector}

One limitation of these local features is that they lack semantics and discriminative capacity. Therefore mid-level and high-level features were proposed (cite TDD).

\textbf{VLAD}

\subsubsection{Classification}

\subsection{State of the Art Approaches using local features}

\subsubsection{Action Recognition by Dense Trajectories -- Wang et al. (2011)}

\textcite{wang_action_2011} introduce a tracking technique called \textit{dense trajectories} for action classification from videos.

Points are sampled densely from each frame and then tracked using a dense optical flow field.

Local features are extracted along the resulting point trajectories to form trajectory descriptors, which are then aggregated into a global video descriptor using the bag-of-words paradigm. cite ??

Before, also other approaches used feature trajectories for action recognition by either tracking sparse spatio-temporal interest points using a standard KLT tracker or by matching SIFT features between consecutive frames.

Dense sampling of interest points has shown to yield improved performance in action recognition over sparse spatio-temporal interest points \cite{wang_evaluation_2009}.
However using the KLT tracker to obtain dense trajectories or matching SIFT features on densely sampled points would be computationally too expensive to handle large datasets. 
The authors approach therefore represents an efficient way to extract dense trajectories.

Since motion in a video, and therefore trajectories can stem from either motion of interest or unwanted camera motion, the authors propose using a descriptor called \textit{Motion Boundary Histogram} (MBH), which aims at focussing on foreground motion.
The motion boundary histogram descriptor is designed to make the classification of actions in a video invariant to camera motion.

The overall approach for obtaining a descriptor along densely extracted trajectories is shown in figure \ref{fig:densetrajectories_approach}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img_conventional/densetrajectories_approach}
    \caption{Description of densely extracted trajectories \cite{wang_action_2011}}
    \label{fig:densetrajectories_approach}
\end{figure}

Dense trajectories are obtained separately from 8 spatial scales, which differ by a factor of $1 / \sqrt{2}$.
Points are sampled on a grid spaced by $W$ pixels on each scale. Experimentally $W = 5$ has been shown to yield good results.

Each point $P_t$ at frame $t$ is tracked to the next frame by mean-filtering a dense optical flow field, which was extracted by the Farnebäck algorithm \cite{farneback_two-frame_2003} as implemented in OpenCV.
The tracked points in subsequent frames then form the trajectory $(P_t, P_{t+1}, P_{t+2}, \cdots)$.

The maximum length of a trajectory is limited to $L = 15$ frames to avoid the problem of drifting.
Trajectories that exceed this limit are removed from the tracking process.
The presence of a trackectory in each $W \times W$ unit of each frame is verified. If no tajectory is present, a new point is sampled and added to the tracking process.

Since only dynamic information is important for action recognition, static trajectories are removed in a pre-processing stage.
Erroneous trajectories with sudden large displacements are also removed.

A simple descriptor is obtained from the shape information given by the trajectory.
It is formed by normalizing the spatial displacements given by the differences of consecutive points in a trajectory.
Formally the \textit{trajectory descriptor} $S'$ is given by:
\begin{equation*}
    S' = \frac{(\Delta P_t, \cdots, \Delta P_{t+L-1})}{\sum_{j=t}^{t+L-1} \|P_j\|}
\end{equation*}
Where $\Delta P_t = (P_{t+1} - P_t) = (x_{t+1} - x_t, y_{t+1} - y_t)$.

\textbf{Local Feature Descriptors:}\\
Local features are extracted from video volumes of size $N \times N \times L$ around the trajectories as depicted in figure \ref{fig:densetrajectories_approach}, where $N = 32$ has shown to yield good results.

Feature descriptors evaluated in the context of dense trajectories are (see also ??):
\begin{itemize}
    \item \textbf{HOG} (Histogram of Oriented Gradients) \cite{dalal_histograms_2005-1}
    \item \textbf{HOF} (Histogram of Optical Flow) \cite{laptev_learning_2008}
    \item \textbf{MBH} (Motion Boundary Histogram) \cite{dalal_human_2006}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img_conventional/densetrajectories_featurevisualization}
    \caption{Visualization of the information captured by HOG, HOF and MBH on complete video frames. In each image, the orientation is given by color, the magnitude is given by saturation. \cite{wang_action_2011}}
    \label{fig:densetrajectories_featurevisualization}
\end{figure}

The HOG descriptor encodes static appearance information by computing the orientations of image gradients and aggregating them in a histogram over all subframes of the current video volume along the trajectory. In this approach the histograms contain 8 bins.

The HOF descriptor aggregates the orientations of optical flow vectors in a histogram and therefore captures local motion information. An additional bin is used here, i.e.\ 9 bins.

The MBH descriptor separately calculates the spatial derivatives of the $x$- and $y$-component of the optical flow field.
The orientations of the derivatives are aggregated into histograms (similarly to the HOG descriptor), which represent the video volume.
An advantage of MBH is that is suppresses constant motion, since it takes only the changes in the flow field (i.e.\ motion boundaries) into account.
The authors therefore use MBH as an easy way to filter noise stemming from background camera-motion (compare the optical flow-image and motion boundaries in figure \ref{fig:densetrajectories_featurevisualization}).

The authors evaluate their approach on the KTH, YouTube, Hollywood2 and UCF-Sports dataset using a standard bag-of-features approach as follows:

\begin{enumerate}
    \item Construction of a codebook for each descriptor-type (trajectory, HOG, HOF and MBH).
        $100.000$ descriptors for each type are randomly chosen from all extracted descriptors over the dataset's training split.
        These descriptors are clustered into a 4000 words long codebook using $k$-means.
    \item Each extracted descriptor from a video is assigned to its nearest codebook-descriptor using the Euclidean distance.
        The number of occurences are aggregated in a histogram, which builds the global video descriptor.
    \item A classifier (here a non-linear SVM with a $\chi^2$ kernel) is trained to assign the class-labels to the global video descriptors.
\end{enumerate}

Different descriptor can be combined ??.

Besides densely sampled trajectories, the authors evaluate baseline trajectories obtained from the KLT tracker for comparison.
The same descriptors (trajectory, HOG, HOF and MBH) are used aorund the KLT-trajectories.

\begin{table}[H]
    \centering
    \includegraphics[width=\textwidth]{img_conventional/densetrajectories_results}
    \caption{Results of dense trajectories compared to KLT-trajectories when using different feature descriptors. \cite{wang_action_2011}}
    \label{tab:densetrajectories_results}
\end{table}

The results in table \ref{tab:densetrajectories_results} show the superiority of dense trajectories compared to the KLT baseline.

The simple trajectory descriptor yields surprisingly good results, which confirms the importance of motion information encoded in the trajectory shapes according to the authors.

The MBH descriptor performs significantly better than all the other descriptors.
On the youtube dataset, the advantage of using the MBH descriptor is most prominent, since the videos in this dataset contain a lot of noise from camera-motion (uncontrolled, realistic videos, often recorded by handheld cameras).

The dense trajectory approach significantly outperformed the state-of-the art on the YouTube, Hollywood2 and UCF Sports datasets when all descriptors are combined.
On the KTH dataset, the approach yields competitive results.

\subsubsection{Action recognition with improved trajectories -- Wang et al. (2013)}
\cite{wang_action_2013}
``Recently, Wang et al. proposed improved Dense Trajectories (iDT) [44] which is currently the state-of-the-art hand-crafted feature'' tran learning spatio-temporal features with 3D convolutional networks.

\subsubsection{Multi-view super vector for action recognition -- Cai et al. (2014)}
\cite{cai_multi-view_2014}

\subsubsection{Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice -- Peng et al. (2014)}
\cite{peng_bag_2014}

\subsection{Beyond gaussian pyramid: Multi-skip feature stacking for action recognition}
UCF 101 - 89.1\%

other iDT-based methods: Beyond gaussian pyramid: Multi-skip feature stacking for action recognition.
Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice.

